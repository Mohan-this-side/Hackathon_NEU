<<<<<<< HEAD
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "train = pd.read_csv(\"/Users/mohan/NEU/Projects_and_Hackathon/stay-or-stray/train.csv\")\n",
    "test = pd.read_csv(\"/Users/mohan/NEU/Projects_and_Hackathon/stay-or-stray/test.csv\")\n",
    "\n",
    "log_train = pd.read_csv(\"/Users/mohan/NEU/Projects_and_Hackathon/stay-or-stray/log_train.csv\")\n",
    "log_test = pd.read_csv(\"/Users/mohan/NEU/Projects_and_Hackathon/stay-or-stray/log_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_train_df= log_train\n",
    "train_df = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/reshape/merge.py:1203: RuntimeWarning: invalid value encountered in cast\n",
      "  if not (lk == lk.astype(rk.dtype))[~np.isnan(lk)].all():\n"
     ]
    }
   ],
   "source": [
    "log_train_df['time'] = pd.to_datetime(log_train_df['time'])\n",
    "\n",
    "# Extract features\n",
    "log_train_df['hour'] = log_train_df['time'].dt.hour\n",
    "log_train_df['day_of_week'] = log_train_df['time'].dt.dayofweek\n",
    "log_train_df['day'] = log_train_df['time'].dt.day\n",
    "\n",
    "# Count of events per ID\n",
    "event_count = log_train_df.groupby('ID')['event'].count().reset_index(name='event_count')\n",
    "\n",
    "# Count of unique sources per ID\n",
    "unique_sources = log_train_df.groupby('ID')['source'].nunique().reset_index(name='unique_sources')\n",
    "\n",
    "# Merge these features back to the original dataset on ID\n",
    "train_df_enriched = train_df.merge(event_count, on='ID', how='left')\n",
    "train_df_enriched = train_df_enriched.merge(unique_sources, on='ID', how='left')\n",
    "\n",
    "\n",
    "# Time difference features (e.g., average time between events for each ID)\n",
    "log_train_df['time_diff'] = log_train_df.sort_values(['ID', 'time']).groupby('ID')['time'].diff().dt.total_seconds()\n",
    "avg_time_diff = log_train_df.groupby('ID')['time_diff'].mean().reset_index(name='avg_time_diff')\n",
    "\n",
    "# Fill missing values for IDs that did not have event data\n",
    "\n",
    "\n",
    "train_df_enriched = train_df_enriched.merge(avg_time_diff, on='ID', how='left')\n",
    "\n",
    "# Fill missing values for IDs that did not have event data\n",
    "train_df_enriched.fillna({'event_count': 0, 'unique_sources': 0, \"avg_time_diff\":0}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightgbm/callback.py:325: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning('Early stopping is not available in dart mode')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's binary_logloss: 0.493656\tvalid_1's binary_logloss: 0.494548\n",
      "[200]\ttraining's binary_logloss: 0.424249\tvalid_1's binary_logloss: 0.425607\n",
      "[300]\ttraining's binary_logloss: 0.362968\tvalid_1's binary_logloss: 0.365108\n",
      "[400]\ttraining's binary_logloss: 0.318325\tvalid_1's binary_logloss: 0.321145\n",
      "[500]\ttraining's binary_logloss: 0.282717\tvalid_1's binary_logloss: 0.286579\n",
      "[600]\ttraining's binary_logloss: 0.271126\tvalid_1's binary_logloss: 0.275693\n",
      "[700]\ttraining's binary_logloss: 0.252292\tvalid_1's binary_logloss: 0.258003\n",
      "[800]\ttraining's binary_logloss: 0.242019\tvalid_1's binary_logloss: 0.248798\n",
      "[900]\ttraining's binary_logloss: 0.229496\tvalid_1's binary_logloss: 0.237716\n",
      "[1000]\ttraining's binary_logloss: 0.222304\tvalid_1's binary_logloss: 0.231764\n",
      "[1100]\ttraining's binary_logloss: 0.214628\tvalid_1's binary_logloss: 0.225572\n",
      "[1200]\ttraining's binary_logloss: 0.210662\tvalid_1's binary_logloss: 0.222829\n",
      "[1300]\ttraining's binary_logloss: 0.205646\tvalid_1's binary_logloss: 0.219315\n",
      "[1400]\ttraining's binary_logloss: 0.198895\tvalid_1's binary_logloss: 0.214466\n",
      "[1500]\ttraining's binary_logloss: 0.196557\tvalid_1's binary_logloss: 0.213399\n",
      "[1600]\ttraining's binary_logloss: 0.191771\tvalid_1's binary_logloss: 0.210375\n",
      "[1700]\ttraining's binary_logloss: 0.187317\tvalid_1's binary_logloss: 0.207872\n",
      "[1800]\ttraining's binary_logloss: 0.183863\tvalid_1's binary_logloss: 0.206277\n",
      "[1900]\ttraining's binary_logloss: 0.180325\tvalid_1's binary_logloss: 0.204736\n",
      "[2000]\ttraining's binary_logloss: 0.17766\tvalid_1's binary_logloss: 0.203849\n",
      "[2100]\ttraining's binary_logloss: 0.173677\tvalid_1's binary_logloss: 0.202254\n",
      "[2200]\ttraining's binary_logloss: 0.170776\tvalid_1's binary_logloss: 0.201389\n",
      "[2300]\ttraining's binary_logloss: 0.167391\tvalid_1's binary_logloss: 0.200336\n",
      "[2400]\ttraining's binary_logloss: 0.163996\tvalid_1's binary_logloss: 0.199499\n",
      "[2500]\ttraining's binary_logloss: 0.161608\tvalid_1's binary_logloss: 0.19911\n",
      "[2600]\ttraining's binary_logloss: 0.159377\tvalid_1's binary_logloss: 0.198819\n",
      "[2700]\ttraining's binary_logloss: 0.157004\tvalid_1's binary_logloss: 0.198469\n",
      "[2800]\ttraining's binary_logloss: 0.155812\tvalid_1's binary_logloss: 0.198557\n",
      "[2900]\ttraining's binary_logloss: 0.153411\tvalid_1's binary_logloss: 0.198179\n",
      "[3000]\ttraining's binary_logloss: 0.150822\tvalid_1's binary_logloss: 0.19773\n",
      "[3100]\ttraining's binary_logloss: 0.148455\tvalid_1's binary_logloss: 0.197363\n",
      "[3200]\ttraining's binary_logloss: 0.146577\tvalid_1's binary_logloss: 0.197197\n",
      "Accuracy: 0.93\n",
      "[[12896  1454]\n",
      " [  582 12853]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93     14350\n",
      "           1       0.90      0.96      0.93     13435\n",
      "\n",
      "    accuracy                           0.93     27785\n",
      "   macro avg       0.93      0.93      0.93     27785\n",
      "weighted avg       0.93      0.93      0.93     27785\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# Assuming 'train_df_enriched' is your dataset\n",
    "#test_features = test_df_enriched.drop('ID', axis=1)\n",
    "train_clean = train_df_enriched.dropna(subset=['label'])\n",
    "\n",
    "selected_features = ['X1', 'X3', 'X5', 'X16', 'X25', 'X28', 'X30', 'X35', 'X36', 'X37',\n",
    "       'X41', 'X42', 'X43', 'X44', 'X45', 'X47', 'X48', 'X49', 'X52', 'X56',\n",
    "       'X57', 'X74', 'X77', 'X78', 'X80', 'X86', 'X87', 'X92', 'X96', 'X99',\n",
    "       'X100', 'X101', 'X105', 'X108', 'X109', 'X111', 'X113', 'X115', 'X116',\n",
    "       'X117', 'X118', 'X121', 'X122', 'X127', 'X128', 'X134', 'X135', 'X136',\n",
    "       'X137', 'X138', 'avg_time_diff']\n",
    "\n",
    "X = train_clean[selected_features]\n",
    "y = train_clean['label']\n",
    "\n",
    "# Initialize the SimpleImputer with 'median' strategy\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X_imputed_df = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "\n",
    "# Resampling\n",
    "X_resampled, y_resampled = ADASYN().fit_resample(X_imputed_df, y)\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Define improved model parameters for 'dart' boosting\n",
    "params = {\n",
    "    'boosting_type': 'dart',  # Use 'dart' boosting type\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 100,\n",
    "    'max_depth': -1,\n",
    "    'min_child_samples': 30,\n",
    "    'max_bin': 255,\n",
    "    'subsample': 0.8,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 0.001,\n",
    "    'subsample_for_bin': 200000,\n",
    "    'min_split_gain': 0.0,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Creating the LightGBM data containers\n",
    "dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "dtest = lgb.Dataset(X_test, label=y_test, reference=dtrain)\n",
    "\n",
    "# Train the model\n",
    "clf = lgb.train(params, dtrain, num_boost_round=3200, valid_sets=[dtrain, dtest],callbacks=[lgb.early_stopping(stopping_rounds=100),lgb.log_evaluation(period=100)])\n",
    "\n",
    "# Predictions\n",
    "predictions = clf.predict(X_test, num_iteration=clf.best_iteration)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, predictions.round())\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(confusion_matrix(y_test, predictions.round()))\n",
    "print(classification_report(y_test, predictions.round()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.926345404703506\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Assuming `predictions` are the predicted probabilities for the positive class from your model\n",
    "# And `y_test` are the true binary labels in the test set\n",
    "\n",
    "# Convert probabilities to binary predictions based on a threshold (e.g., 0.5)\n",
    "binary_predictions = [1 if prob > 0.5 else 0 for prob in predictions]\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, binary_predictions)\n",
    "\n",
    "print(f'F1 Score: {f1}')\n",
    "#F1_Score : 0.9221600231637873 \n",
    "#F1_Score : 0.9229871645274212 - 0.1 median\n",
    "#F1_Score : 0.9242930498300427 - mean lightbgm\n",
    "#F1_Score : 0.9243412961382085 - median lightbgm\n",
    "#F1_Score : 0.9246502317282795 - lightbgm median learning rate= 0.01\n",
    "#F1_Score : 0.9207335755241388 - lightbgm median learning rate= 0.01 dart\n",
    "#F1 Score: 0.9220892274211099 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100\n",
    "#F1 Score: 0.924385564845984 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=1400\n",
    "#F1 Score: 0.9251454801749377 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=1800\n",
    "#F1 Score: 0.9258336042773021 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=2200\n",
    "#F1 Score: 0.9257118080647493 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=2000\n",
    "#F1 Score: 0.9261367730777545 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=2000 - LR - 0.066\n",
    "#F1 Score: 0.9253763595764605 - lightbgm median learning rate= 0.01 gbdt - with stopping_rounds=100 - num_boost_round=2000 - LR - 0.066\n",
    "#F1 Score: 0.9266568058010751 - lightbgm median learning rate= 0.01 gbdt - with stopping_rounds=100 - num_boost_round=700 - LR - 0.066\n",
    "#F1 Score: 0.9263788190311294 - lightbgm median learning rate= 0.01 gbdt - with stopping_rounds=100 - num_boost_round=700 - LR - 0.066 -num lev 50\n",
    "#F1 Score: 0.9257345235083662 - lightbgm median lr= 0.01 dart - num_boost_round=2000 -50NL **\n",
    "#F1 Score: 0.9256598187529336 - lightbgm median lr= 0.01 dart - num_boost_round=2400 -50NL \n",
    "#F1 Score: 0.9257774406761295 - lightbgm median lr= 0.01 dart - num_boost_round=2200 -50NL ***\n",
    "#F1 Score: 0.926345404703506 - lightbgm median lr= 0.01 dart - num_boost_round=2200 -50NL - imp_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.9698888726076903\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Assuming `predictions` are the predicted probabilities for the positive class from your model\n",
    "# And `y_test` are the true binary labels in the test set\n",
    "\n",
    "# Calculate AUC score\n",
    "auc_score = roc_auc_score(y_test, predictions)\n",
    "\n",
    "print(f'AUC Score: {auc_score}')\n",
    "#AUC Score: 0.9234141033971461 - lightbgm median learning rate= 0.01\n",
    "#AUC Score: 0.9198248761840062 - lightbgm median learning rate= 0.01 dart\n",
    "#AUC Score: 0.9646368563832868 - lightbgm median learning rate= 0.01 dart without fold\n",
    "#AUC Score: 0.9645827863163663 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100\n",
    "#AUC Score: 0.9659848237540357 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=1400\n",
    "#AUC Score: 0.9666748373275778 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=1800\n",
    "#AUC Score: 0.9670271115081299 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=2200\n",
    "#AUC Score: 0.9668365984505018 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=2000\n",
    "#AUC Score: 0.9670062293886964 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=2000 - LR - 0.066\n",
    "#AUC Score: 0.9667961485857925 - lightbgm median learning rate= 0.01 gbdt - with stopping_rounds=100 - num_boost_round=2000 - LR - 0.066\n",
    "#AUC Score: 0.9677003777642768 - lightbgm median learning rate= 0.01 gbdt - with stopping_rounds=100 - num_boost_round=700 - LR - 0.066\n",
    "#AUC Score: 0.9670806886847413 - lightbgm median learning rate= 0.01 gbdt - with stopping_rounds=100 - num_boost_round=700 - LR - 0.066 -num lev 50\n",
    "#AUC Score: 0.9670328673716297 - lightbgm median lr= 0.01 dart - num_boost_round=2000 -50NL **\n",
    "#AUC Score: 0.9672647174965239 - lightbgm median lr= 0.01 dart - num_boost_round=2400 -50NL \n",
    "#AUC Score: 0.9672058554404861 - lightbgm median lr= 0.01 dart - num_boost_round=2200 -50NL ***\n",
    "#AUC Score: 0.9698888726076903 - lightbgm median lr= 0.01 dart - num_boost_round=2200 -50NL - imp_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_test_df=log_test\n",
    "test_df = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_test_df['time'] = pd.to_datetime(log_test_df['time'])\n",
    "\n",
    "# Extract features\n",
    "log_test_df['hour'] = log_test_df['time'].dt.hour\n",
    "log_test_df['day_of_week'] = log_test_df['time'].dt.dayofweek\n",
    "log_test_df['day'] = log_test_df['time'].dt.day\n",
    "\n",
    "# Count of events per ID\n",
    "event_count = log_test_df.groupby('ID')['event'].count().reset_index(name='event_count')\n",
    "\n",
    "# Count of unique sources per ID\n",
    "unique_sources = log_test_df.groupby('ID')['source'].nunique().reset_index(name='unique_sources')\n",
    "\n",
    "# Merge these features back to the original dataset on ID\n",
    "test_df_enriched = test_df.merge(event_count, on='ID', how='left')\n",
    "test_df_enriched = test_df_enriched.merge(unique_sources, on='ID', how='left')\n",
    "\n",
    "\n",
    "\n",
    "# Time difference features (e.g., average time between events for each ID)\n",
    "log_test_df['time_diff'] = log_test_df.sort_values(['ID', 'time']).groupby('ID')['time'].diff().dt.total_seconds()\n",
    "avg_time_diff = log_test_df.groupby('ID')['time_diff'].mean().reset_index(name='avg_time_diff')\n",
    "\n",
    "\n",
    "\n",
    "test_df_enriched = test_df_enriched.merge(avg_time_diff, on='ID', how='left')\n",
    "\n",
    "# Fill missing values for IDs that did not have event data\n",
    "test_df_enriched.fillna({'event_count': 0, 'unique_sources': 0, 'avg_time_diff':0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ID  LABEL\n",
      "0   17547      0\n",
      "1  140449      1\n",
      "2  182658      1\n",
      "3  149652      1\n",
      "4  106304      1\n"
     ]
    }
   ],
   "source": [
    "# Assuming `test_df_enriched` is your test dataset and `clf` is your trained LightGBM model\n",
    "\n",
    "# Convert the 'ID' column from float to int\n",
    "test_df_enriched['ID'] = test_df_enriched['ID'].astype(int)\n",
    "\n",
    "# Separate out the 'ID' column and features from the 'test' dataset\n",
    "#test_features = test_df_enriched.drop('ID', axis=1)\n",
    "test_features = test_df_enriched[selected_features]\n",
    "\n",
    "# Impute the 'test' dataset using the fitted SimpleImputer\n",
    "# Note: We use .transform() here, NOT .fit_transform()\n",
    "test_imputed = imputer.transform(test_features)\n",
    "\n",
    "# Predict the labels using the trained LightGBM model\n",
    "predicted_labels = clf.predict(test_imputed)\n",
    "\n",
    "# For binary classification, LightGBM might output probabilities. If so, convert these to 0 or 1\n",
    "# Uncomment the following line if your model outputs probabilities\n",
    "predicted_labels = np.round(predicted_labels).astype(int)\n",
    "\n",
    "# Create a DataFrame with 'ID' and the predicted 'label'\n",
    "results_df = pd.DataFrame({\n",
    "    'ID': test_df_enriched['ID'],\n",
    "    'LABEL': predicted_labels\n",
    "})\n",
    "\n",
    "# Optionally, convert the 'LABEL' column to the desired format, e.g., int\n",
    "# results_df['LABEL'] = results_df['LABEL'].astype(int)\n",
    "\n",
    "# Output the DataFrame with 'ID' and 'label'\n",
    "# Save to a CSV file\n",
    "results_df.to_csv('/Users/mohan/NEU/Projects_and_Hackathon/stay-or-stray/Submissions/my_submission_35_lGBM_ADAS_mdn_0.01LR_dart_wo_fold_2200BR_50NL_impf.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(results_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
||||||| parent of 7f5be69 (uploading the all codes)
=======
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "train = pd.read_csv(\"/Users/mohan/NEU/Projects_and_Hackathon/stay-or-stray/train.csv\")\n",
    "test = pd.read_csv(\"/Users/mohan/NEU/Projects_and_Hackathon/stay-or-stray/test.csv\")\n",
    "\n",
    "log_train = pd.read_csv(\"/Users/mohan/NEU/Projects_and_Hackathon/stay-or-stray/log_train.csv\")\n",
    "log_test = pd.read_csv(\"/Users/mohan/NEU/Projects_and_Hackathon/stay-or-stray/log_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_train_df= log_train\n",
    "train_df = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/reshape/merge.py:1203: RuntimeWarning: invalid value encountered in cast\n",
      "  if not (lk == lk.astype(rk.dtype))[~np.isnan(lk)].all():\n"
     ]
    }
   ],
   "source": [
    "log_train_df['time'] = pd.to_datetime(log_train_df['time'])\n",
    "\n",
    "# Extract features\n",
    "log_train_df['hour'] = log_train_df['time'].dt.hour\n",
    "log_train_df['day_of_week'] = log_train_df['time'].dt.dayofweek\n",
    "log_train_df['day'] = log_train_df['time'].dt.day\n",
    "\n",
    "# Count of events per ID\n",
    "event_count = log_train_df.groupby('ID')['event'].count().reset_index(name='event_count')\n",
    "\n",
    "# Count of unique sources per ID\n",
    "unique_sources = log_train_df.groupby('ID')['source'].nunique().reset_index(name='unique_sources')\n",
    "\n",
    "# Merge these features back to the original dataset on ID\n",
    "train_df_enriched = train_df.merge(event_count, on='ID', how='left')\n",
    "train_df_enriched = train_df_enriched.merge(unique_sources, on='ID', how='left')\n",
    "\n",
    "\n",
    "# Time difference features (e.g., average time between events for each ID)\n",
    "log_train_df['time_diff'] = log_train_df.sort_values(['ID', 'time']).groupby('ID')['time'].diff().dt.total_seconds()\n",
    "avg_time_diff = log_train_df.groupby('ID')['time_diff'].mean().reset_index(name='avg_time_diff')\n",
    "\n",
    "# Fill missing values for IDs that did not have event data\n",
    "\n",
    "\n",
    "train_df_enriched = train_df_enriched.merge(avg_time_diff, on='ID', how='left')\n",
    "\n",
    "# Fill missing values for IDs that did not have event data\n",
    "train_df_enriched.fillna({'event_count': 0, 'unique_sources': 0, \"avg_time_diff\":0}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/lightgbm/callback.py:325: UserWarning: Early stopping is not available in dart mode\n",
      "  _log_warning('Early stopping is not available in dart mode')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's binary_logloss: 0.504834\tvalid_1's binary_logloss: 0.505343\n",
      "[200]\ttraining's binary_logloss: 0.438737\tvalid_1's binary_logloss: 0.439483\n",
      "[300]\ttraining's binary_logloss: 0.379108\tvalid_1's binary_logloss: 0.379913\n",
      "[400]\ttraining's binary_logloss: 0.3352\tvalid_1's binary_logloss: 0.3361\n",
      "[500]\ttraining's binary_logloss: 0.298913\tvalid_1's binary_logloss: 0.299922\n",
      "[600]\ttraining's binary_logloss: 0.286945\tvalid_1's binary_logloss: 0.288138\n",
      "[700]\ttraining's binary_logloss: 0.268117\tvalid_1's binary_logloss: 0.269494\n",
      "[800]\ttraining's binary_logloss: 0.25749\tvalid_1's binary_logloss: 0.259215\n",
      "[900]\ttraining's binary_logloss: 0.244825\tvalid_1's binary_logloss: 0.246963\n",
      "[1000]\ttraining's binary_logloss: 0.237313\tvalid_1's binary_logloss: 0.239915\n",
      "[1100]\ttraining's binary_logloss: 0.229455\tvalid_1's binary_logloss: 0.232534\n",
      "[1200]\ttraining's binary_logloss: 0.225687\tvalid_1's binary_logloss: 0.229271\n",
      "[1300]\ttraining's binary_logloss: 0.220737\tvalid_1's binary_logloss: 0.224935\n",
      "[1400]\ttraining's binary_logloss: 0.214523\tvalid_1's binary_logloss: 0.219513\n",
      "[1500]\ttraining's binary_logloss: 0.21254\tvalid_1's binary_logloss: 0.218127\n",
      "[1600]\ttraining's binary_logloss: 0.208283\tvalid_1's binary_logloss: 0.214727\n",
      "[1700]\ttraining's binary_logloss: 0.204496\tvalid_1's binary_logloss: 0.211888\n",
      "[1800]\ttraining's binary_logloss: 0.201735\tvalid_1's binary_logloss: 0.209982\n",
      "[1900]\ttraining's binary_logloss: 0.19888\tvalid_1's binary_logloss: 0.208142\n",
      "[2000]\ttraining's binary_logloss: 0.196884\tvalid_1's binary_logloss: 0.207132\n",
      "[2100]\ttraining's binary_logloss: 0.193882\tvalid_1's binary_logloss: 0.205337\n",
      "[2200]\ttraining's binary_logloss: 0.191758\tvalid_1's binary_logloss: 0.204295\n",
      "Accuracy: 0.93\n",
      "[[12902  1448]\n",
      " [  594 12841]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93     14350\n",
      "           1       0.90      0.96      0.93     13435\n",
      "\n",
      "    accuracy                           0.93     27785\n",
      "   macro avg       0.93      0.93      0.93     27785\n",
      "weighted avg       0.93      0.93      0.93     27785\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# Assuming 'train_df_enriched' is your dataset\n",
    "#test_features = test_df_enriched.drop('ID', axis=1)\n",
    "train_clean = train_df_enriched.dropna(subset=['label'])\n",
    "\n",
    "selected_features = ['X1', 'X3', 'X5', 'X16', 'X25', 'X28', 'X30', 'X35', 'X36', 'X37',\n",
    "       'X41', 'X42', 'X43', 'X44', 'X45', 'X47', 'X48', 'X49', 'X52', 'X56',\n",
    "       'X57', 'X74', 'X77', 'X78', 'X80', 'X86', 'X87', 'X92', 'X96', 'X99',\n",
    "       'X100', 'X101', 'X105', 'X108', 'X109', 'X111', 'X113', 'X115', 'X116',\n",
    "       'X117', 'X118', 'X121', 'X122', 'X127', 'X128', 'X134', 'X135', 'X136',\n",
    "       'X137', 'X138', 'avg_time_diff']\n",
    "\n",
    "X = train_clean[selected_features]\n",
    "y = train_clean['label']\n",
    "\n",
    "# Initialize the SimpleImputer with 'median' strategy\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X_imputed_df = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "\n",
    "# Resampling\n",
    "X_resampled, y_resampled = ADASYN().fit_resample(X_imputed_df, y)\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Define improved model parameters for 'dart' boosting\n",
    "params = {\n",
    "    'boosting_type': 'dart',  # Use 'dart' boosting type\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 50,\n",
    "    'max_depth': -1,\n",
    "    'min_child_samples': 30,\n",
    "    'max_bin': 255,\n",
    "    'subsample': 0.8,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 0.001,\n",
    "    'subsample_for_bin': 200000,\n",
    "    'min_split_gain': 0.0,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Creating the LightGBM data containers\n",
    "dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "dtest = lgb.Dataset(X_test, label=y_test, reference=dtrain)\n",
    "\n",
    "# Train the model\n",
    "clf = lgb.train(params, dtrain, num_boost_round=2200, valid_sets=[dtrain, dtest],callbacks=[lgb.early_stopping(stopping_rounds=100),lgb.log_evaluation(period=100)])\n",
    "\n",
    "# Predictions\n",
    "predictions = clf.predict(X_test, num_iteration=clf.best_iteration)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, predictions.round())\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(confusion_matrix(y_test, predictions.round()))\n",
    "print(classification_report(y_test, predictions.round()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.926345404703506\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Assuming `predictions` are the predicted probabilities for the positive class from your model\n",
    "# And `y_test` are the true binary labels in the test set\n",
    "\n",
    "# Convert probabilities to binary predictions based on a threshold (e.g., 0.5)\n",
    "binary_predictions = [1 if prob > 0.5 else 0 for prob in predictions]\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, binary_predictions)\n",
    "\n",
    "print(f'F1 Score: {f1}')\n",
    "#F1_Score : 0.9221600231637873 \n",
    "#F1_Score : 0.9229871645274212 - 0.1 median\n",
    "#F1_Score : 0.9242930498300427 - mean lightbgm\n",
    "#F1_Score : 0.9243412961382085 - median lightbgm\n",
    "#F1_Score : 0.9246502317282795 - lightbgm median learning rate= 0.01\n",
    "#F1_Score : 0.9207335755241388 - lightbgm median learning rate= 0.01 dart\n",
    "#F1 Score: 0.9220892274211099 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100\n",
    "#F1 Score: 0.924385564845984 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=1400\n",
    "#F1 Score: 0.9251454801749377 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=1800\n",
    "#F1 Score: 0.9258336042773021 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=2200\n",
    "#F1 Score: 0.9257118080647493 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=2000\n",
    "#F1 Score: 0.9261367730777545 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=2000 - LR - 0.066\n",
    "#F1 Score: 0.9253763595764605 - lightbgm median learning rate= 0.01 gbdt - with stopping_rounds=100 - num_boost_round=2000 - LR - 0.066\n",
    "#F1 Score: 0.9266568058010751 - lightbgm median learning rate= 0.01 gbdt - with stopping_rounds=100 - num_boost_round=700 - LR - 0.066\n",
    "#F1 Score: 0.9263788190311294 - lightbgm median learning rate= 0.01 gbdt - with stopping_rounds=100 - num_boost_round=700 - LR - 0.066 -num lev 50\n",
    "#F1 Score: 0.9257345235083662 - lightbgm median lr= 0.01 dart - num_boost_round=2000 -50NL **\n",
    "#F1 Score: 0.9256598187529336 - lightbgm median lr= 0.01 dart - num_boost_round=2400 -50NL \n",
    "#F1 Score: 0.9257774406761295 - lightbgm median lr= 0.01 dart - num_boost_round=2200 -50NL ***\n",
    "#F1 Score: 0.926345404703506 - lightbgm median lr= 0.01 dart - num_boost_round=2200 -50NL - imp_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.9698888726076903\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Assuming `predictions` are the predicted probabilities for the positive class from your model\n",
    "# And `y_test` are the true binary labels in the test set\n",
    "\n",
    "# Calculate AUC score\n",
    "auc_score = roc_auc_score(y_test, predictions)\n",
    "\n",
    "print(f'AUC Score: {auc_score}')\n",
    "#AUC Score: 0.9234141033971461 - lightbgm median learning rate= 0.01\n",
    "#AUC Score: 0.9198248761840062 - lightbgm median learning rate= 0.01 dart\n",
    "#AUC Score: 0.9646368563832868 - lightbgm median learning rate= 0.01 dart without fold\n",
    "#AUC Score: 0.9645827863163663 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100\n",
    "#AUC Score: 0.9659848237540357 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=1400\n",
    "#AUC Score: 0.9666748373275778 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=1800\n",
    "#AUC Score: 0.9670271115081299 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=2200\n",
    "#AUC Score: 0.9668365984505018 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=2000\n",
    "#AUC Score: 0.9670062293886964 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=2000 - LR - 0.066\n",
    "#AUC Score: 0.9667961485857925 - lightbgm median learning rate= 0.01 gbdt - with stopping_rounds=100 - num_boost_round=2000 - LR - 0.066\n",
    "#AUC Score: 0.9677003777642768 - lightbgm median learning rate= 0.01 gbdt - with stopping_rounds=100 - num_boost_round=700 - LR - 0.066\n",
    "#AUC Score: 0.9670806886847413 - lightbgm median learning rate= 0.01 gbdt - with stopping_rounds=100 - num_boost_round=700 - LR - 0.066 -num lev 50\n",
    "#AUC Score: 0.9670328673716297 - lightbgm median lr= 0.01 dart - num_boost_round=2000 -50NL **\n",
    "#AUC Score: 0.9672647174965239 - lightbgm median lr= 0.01 dart - num_boost_round=2400 -50NL \n",
    "#AUC Score: 0.9672058554404861 - lightbgm median lr= 0.01 dart - num_boost_round=2200 -50NL ***\n",
    "#AUC Score: 0.9698888726076903 - lightbgm median lr= 0.01 dart - num_boost_round=2200 -50NL - imp_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_test_df=log_test\n",
    "test_df = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_test_df['time'] = pd.to_datetime(log_test_df['time'])\n",
    "\n",
    "# Extract features\n",
    "log_test_df['hour'] = log_test_df['time'].dt.hour\n",
    "log_test_df['day_of_week'] = log_test_df['time'].dt.dayofweek\n",
    "log_test_df['day'] = log_test_df['time'].dt.day\n",
    "\n",
    "# Count of events per ID\n",
    "event_count = log_test_df.groupby('ID')['event'].count().reset_index(name='event_count')\n",
    "\n",
    "# Count of unique sources per ID\n",
    "unique_sources = log_test_df.groupby('ID')['source'].nunique().reset_index(name='unique_sources')\n",
    "\n",
    "# Merge these features back to the original dataset on ID\n",
    "test_df_enriched = test_df.merge(event_count, on='ID', how='left')\n",
    "test_df_enriched = test_df_enriched.merge(unique_sources, on='ID', how='left')\n",
    "\n",
    "\n",
    "\n",
    "# Time difference features (e.g., average time between events for each ID)\n",
    "log_test_df['time_diff'] = log_test_df.sort_values(['ID', 'time']).groupby('ID')['time'].diff().dt.total_seconds()\n",
    "avg_time_diff = log_test_df.groupby('ID')['time_diff'].mean().reset_index(name='avg_time_diff')\n",
    "\n",
    "\n",
    "\n",
    "test_df_enriched = test_df_enriched.merge(avg_time_diff, on='ID', how='left')\n",
    "\n",
    "# Fill missing values for IDs that did not have event data\n",
    "test_df_enriched.fillna({'event_count': 0, 'unique_sources': 0, 'avg_time_diff':0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ID  LABEL\n",
      "0   17547      0\n",
      "1  140449      1\n",
      "2  182658      1\n",
      "3  149652      1\n",
      "4  106304      1\n"
     ]
    }
   ],
   "source": [
    "# Assuming `test_df_enriched` is your test dataset and `clf` is your trained LightGBM model\n",
    "\n",
    "# Convert the 'ID' column from float to int\n",
    "test_df_enriched['ID'] = test_df_enriched['ID'].astype(int)\n",
    "\n",
    "# Separate out the 'ID' column and features from the 'test' dataset\n",
    "#test_features = test_df_enriched.drop('ID', axis=1)\n",
    "test_features = test_df_enriched[selected_features]\n",
    "\n",
    "# Impute the 'test' dataset using the fitted SimpleImputer\n",
    "# Note: We use .transform() here, NOT .fit_transform()\n",
    "test_imputed = imputer.transform(test_features)\n",
    "\n",
    "# Predict the labels using the trained LightGBM model\n",
    "predicted_labels = clf.predict(test_imputed)\n",
    "\n",
    "# For binary classification, LightGBM might output probabilities. If so, convert these to 0 or 1\n",
    "# Uncomment the following line if your model outputs probabilities\n",
    "predicted_labels = np.round(predicted_labels).astype(int)\n",
    "\n",
    "# Create a DataFrame with 'ID' and the predicted 'label'\n",
    "results_df = pd.DataFrame({\n",
    "    'ID': test_df_enriched['ID'],\n",
    "    'LABEL': predicted_labels\n",
    "})\n",
    "\n",
    "# Optionally, convert the 'LABEL' column to the desired format, e.g., int\n",
    "# results_df['LABEL'] = results_df['LABEL'].astype(int)\n",
    "\n",
    "# Output the DataFrame with 'ID' and 'label'\n",
    "# Save to a CSV file\n",
    "results_df.to_csv('/Users/mohan/NEU/Projects_and_Hackathon/stay-or-stray/Submissions/my_submission_35_lGBM_ADAS_mdn_0.01LR_dart_wo_fold_2200BR_50NL_impf.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(results_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
>>>>>>> 7f5be69 (uploading the all codes)
