{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "train = pd.read_csv(\"/Users/mohan/NEU/Projects_and_Hackathon/stay-or-stray/train.csv\")\n",
    "test = pd.read_csv(\"/Users/mohan/NEU/Projects_and_Hackathon/stay-or-stray/test.csv\")\n",
    "\n",
    "log_train = pd.read_csv(\"/Users/mohan/NEU/Projects_and_Hackathon/stay-or-stray/log_train.csv\")\n",
    "log_test = pd.read_csv(\"/Users/mohan/NEU/Projects_and_Hackathon/stay-or-stray/log_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_train_df= log_train\n",
    "train_df = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/reshape/merge.py:1203: RuntimeWarning: invalid value encountered in cast\n",
      "  if not (lk == lk.astype(rk.dtype))[~np.isnan(lk)].all():\n"
     ]
    }
   ],
   "source": [
    "log_train_df['time'] = pd.to_datetime(log_train_df['time'])\n",
    "\n",
    "# Extract features\n",
    "log_train_df['hour'] = log_train_df['time'].dt.hour\n",
    "log_train_df['day_of_week'] = log_train_df['time'].dt.dayofweek\n",
    "log_train_df['day'] = log_train_df['time'].dt.day\n",
    "\n",
    "# Count of events per ID\n",
    "event_count = log_train_df.groupby('ID')['event'].count().reset_index(name='event_count')\n",
    "\n",
    "# Count of unique sources per ID\n",
    "unique_sources = log_train_df.groupby('ID')['source'].nunique().reset_index(name='unique_sources')\n",
    "\n",
    "# Merge these features back to the original dataset on ID\n",
    "train_df_enriched = train_df.merge(event_count, on='ID', how='left')\n",
    "train_df_enriched = train_df_enriched.merge(unique_sources, on='ID', how='left')\n",
    "\n",
    "\n",
    "# Time difference features (e.g., average time between events for each ID)\n",
    "log_train_df['time_diff'] = log_train_df.sort_values(['ID', 'time']).groupby('ID')['time'].diff().dt.total_seconds()\n",
    "avg_time_diff = log_train_df.groupby('ID')['time_diff'].mean().reset_index(name='avg_time_diff')\n",
    "\n",
    "# Fill missing values for IDs that did not have event data\n",
    "\n",
    "\n",
    "train_df_enriched = train_df_enriched.merge(avg_time_diff, on='ID', how='left')\n",
    "\n",
    "# Fill missing values for IDs that did not have event data\n",
    "train_df_enriched.fillna({'event_count': 0, 'unique_sources': 0, \"avg_time_diff\":0}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.340065\tvalid_1's binary_logloss: 0.354743\n",
      "[200]\ttraining's binary_logloss: 0.231144\tvalid_1's binary_logloss: 0.258733\n",
      "[300]\ttraining's binary_logloss: 0.183291\tvalid_1's binary_logloss: 0.225439\n",
      "[400]\ttraining's binary_logloss: 0.154296\tvalid_1's binary_logloss: 0.212362\n",
      "[500]\ttraining's binary_logloss: 0.132248\tvalid_1's binary_logloss: 0.206182\n",
      "[600]\ttraining's binary_logloss: 0.114604\tvalid_1's binary_logloss: 0.203174\n",
      "[700]\ttraining's binary_logloss: 0.100082\tvalid_1's binary_logloss: 0.201868\n",
      "[800]\ttraining's binary_logloss: 0.088054\tvalid_1's binary_logloss: 0.201209\n",
      "[900]\ttraining's binary_logloss: 0.0779067\tvalid_1's binary_logloss: 0.201196\n",
      "Early stopping, best iteration is:\n",
      "[852]\ttraining's binary_logloss: 0.0825997\tvalid_1's binary_logloss: 0.201085\n",
      "Accuracy: 0.92\n",
      "[[12168  1414]\n",
      " [  613 12831]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92     13582\n",
      "           1       0.90      0.95      0.93     13444\n",
      "\n",
      "    accuracy                           0.92     27026\n",
      "   macro avg       0.93      0.93      0.92     27026\n",
      "weighted avg       0.93      0.92      0.92     27026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# Assuming 'train_df_enriched' is your dataset\n",
    "train_clean = train_df_enriched.dropna(subset=['label'])\n",
    "\n",
    "X = train_clean.drop(['label', 'ID'], axis=1)\n",
    "y = train_clean['label']\n",
    "\n",
    "# Initialize the SimpleImputer with 'median' strategy\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X_imputed_df = pd.DataFrame(X_imputed, columns=X.columns)\n",
    "\n",
    "# Resampling\n",
    "X_resampled, y_resampled = ADASYN().fit_resample(X_imputed_df, y)\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Define improved model parameters for 'dart' boosting\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',  \n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 300,\n",
    "    'max_depth': -1,\n",
    "    'min_child_samples': 30,\n",
    "    'max_bin': 255,\n",
    "    'subsample': 0.8,\n",
    "    'subsample_freq': 1,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 0.001,\n",
    "    'subsample_for_bin': 200000,\n",
    "    'min_split_gain': 0.0,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Creating the LightGBM data containers\n",
    "dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "dtest = lgb.Dataset(X_test, label=y_test, reference=dtrain)\n",
    "\n",
    "# Train the model\n",
    "clf = lgb.train(params, dtrain, num_boost_round=2200, valid_sets=[dtrain, dtest],callbacks=[lgb.early_stopping(stopping_rounds=100),lgb.log_evaluation(period=100)])\n",
    "\n",
    "# Predictions\n",
    "predictions = clf.predict(X_test, num_iteration=clf.best_iteration)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, predictions.round())\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(confusion_matrix(y_test, predictions.round()))\n",
    "print(classification_report(y_test, predictions.round()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9267940337318069\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Assuming `predictions` are the predicted probabilities for the positive class from your model\n",
    "# And `y_test` are the true binary labels in the test set\n",
    "\n",
    "# Convert probabilities to binary predictions based on a threshold (e.g., 0.5)\n",
    "binary_predictions = [1 if prob > 0.5 else 0 for prob in predictions]\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, binary_predictions)\n",
    "\n",
    "print(f'F1 Score: {f1}')\n",
    "#F1_Score : 0.9221600231637873 \n",
    "#F1_Score : 0.9229871645274212 - 0.1 median\n",
    "#F1_Score : 0.9242930498300427 - mean lightbgm\n",
    "#F1_Score : 0.9243412961382085 - median lightbgm\n",
    "#F1_Score : 0.9246502317282795 - lightbgm median learning rate= 0.01\n",
    "#F1_Score : 0.9207335755241388 - lightbgm median learning rate= 0.01 dart\n",
    "#F1 Score: 0.9220892274211099 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100\n",
    "#F1 Score: 0.924385564845984 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=1400\n",
    "#F1 Score: 0.9251454801749377 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=1800\n",
    "#F1 Score: 0.9258336042773021 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=2200\n",
    "#F1 Score: 0.9257118080647493 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=2000\n",
    "#F1 Score: 0.9261367730777545 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=2000 - LR - 0.066\n",
    "#F1 Score: 0.9253763595764605 - lightbgm median learning rate= 0.01 gbdt - with stopping_rounds=100 - num_boost_round=2000 - LR - 0.066\n",
    "#F1 Score: 0.9266568058010751 - lightbgm median learning rate= 0.01 gbdt - with stopping_rounds=100 - num_boost_round=700 - LR - 0.066\n",
    "#F1 Score: 0.9263788190311294 - lightbgm median learning rate= 0.01 gbdt - with stopping_rounds=100 - num_boost_round=700 - LR - 0.066 -num lev 50\n",
    "#F1 Score: 0.9257345235083662 - lightbgm median lr= 0.01 dart - num_boost_round=2000 -50NL **\n",
    "#F1 Score: 0.9256598187529336 - lightbgm median lr= 0.01 dart - num_boost_round=2400 -50NL \n",
    "#F1 Score: 0.9257774406761295 - lightbgm median lr= 0.01 dart - num_boost_round=2200 -50NL *** -> score: 0.87251\n",
    "#F1 Score: 0.926345404703506 - lightbgm median lr= 0.01 dart - num_boost_round=2200 -50NL - imp_features\n",
    "#F1 Score: 0.9256610316428262 - lightbgm median lr= 0.01 dart - num_boost_round=2200 -50NL *** -> score:0.87217\n",
    "#F1 Score: 0.9254582190792322 - lightbgm median lr= 0.01 gbdt - num_boost_round=2200 -50NL *** -> score:0.87048\n",
    "#F1 Score: 0.9267940337318069 - lightbgm median lr= 0.01 gbdt - num_boost_round=2200 -300NL *** -> score:0.87048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.9686672259182667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Assuming `predictions` are the predicted probabilities for the positive class from your model\n",
    "# And `y_test` are the true binary labels in the test set\n",
    "\n",
    "# Calculate AUC score\n",
    "auc_score = roc_auc_score(y_test, predictions)\n",
    "\n",
    "print(f'AUC Score: {auc_score}')\n",
    "#AUC Score: 0.9234141033971461 - lightbgm median learning rate= 0.01\n",
    "#AUC Score: 0.9198248761840062 - lightbgm median learning rate= 0.01 dart\n",
    "#AUC Score: 0.9646368563832868 - lightbgm median learning rate= 0.01 dart without fold\n",
    "#AUC Score: 0.9645827863163663 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100\n",
    "#AUC Score: 0.9659848237540357 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=1400\n",
    "#AUC Score: 0.9666748373275778 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=1800\n",
    "#AUC Score: 0.9670271115081299 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=2200\n",
    "#AUC Score: 0.9668365984505018 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=2000\n",
    "#AUC Score: 0.9670062293886964 - lightbgm median learning rate= 0.01 dart - with stopping_rounds=100 - num_boost_round=2000 - LR - 0.066\n",
    "#AUC Score: 0.9667961485857925 - lightbgm median learning rate= 0.01 gbdt - with stopping_rounds=100 - num_boost_round=2000 - LR - 0.066\n",
    "#AUC Score: 0.9677003777642768 - lightbgm median learning rate= 0.01 gbdt - with stopping_rounds=100 - num_boost_round=700 - LR - 0.066\n",
    "#AUC Score: 0.9670806886847413 - lightbgm median learning rate= 0.01 gbdt - with stopping_rounds=100 - num_boost_round=700 - LR - 0.066 -num lev 50\n",
    "#AUC Score: 0.9670328673716297 - lightbgm median lr= 0.01 dart - num_boost_round=2000 -50NL **\n",
    "#AUC Score: 0.9672647174965239 - lightbgm median lr= 0.01 dart - num_boost_round=2400 -50NL \n",
    "#AUC Score: 0.9672058554404861 - lightbgm median lr= 0.01 dart - num_boost_round=2200 -50NL ***\n",
    "#AUC Score: 0.9698888726076903 - lightbgm median lr= 0.01 dart - num_boost_round=2200 -50NL - imp_features\n",
    "#AUC Score: 0.9671630177960566 - lightbgm median lr= 0.01 dart - num_boost_round=2200 -50NL *** -> score:0.87217\n",
    "#AUC Score: 0.9674264074241812 - lightbgm median lr= 0.01 gbdt - num_boost_round=2200 -50NL *** -> score:0.87048\n",
    "#AUC Score: 0.9686672259182667 - lightbgm median lr= 0.01 gbdt - num_boost_round=2200 -300NL *** -> score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_test_df=log_test\n",
    "test_df = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_test_df['time'] = pd.to_datetime(log_test_df['time'])\n",
    "\n",
    "# Extract features\n",
    "log_test_df['hour'] = log_test_df['time'].dt.hour\n",
    "log_test_df['day_of_week'] = log_test_df['time'].dt.dayofweek\n",
    "log_test_df['day'] = log_test_df['time'].dt.day\n",
    "\n",
    "# Count of events per ID\n",
    "event_count = log_test_df.groupby('ID')['event'].count().reset_index(name='event_count')\n",
    "\n",
    "# Count of unique sources per ID\n",
    "unique_sources = log_test_df.groupby('ID')['source'].nunique().reset_index(name='unique_sources')\n",
    "\n",
    "# Merge these features back to the original dataset on ID\n",
    "test_df_enriched = test_df.merge(event_count, on='ID', how='left')\n",
    "test_df_enriched = test_df_enriched.merge(unique_sources, on='ID', how='left')\n",
    "\n",
    "\n",
    "\n",
    "# Time difference features (e.g., average time between events for each ID)\n",
    "log_test_df['time_diff'] = log_test_df.sort_values(['ID', 'time']).groupby('ID')['time'].diff().dt.total_seconds()\n",
    "avg_time_diff = log_test_df.groupby('ID')['time_diff'].mean().reset_index(name='avg_time_diff')\n",
    "\n",
    "\n",
    "\n",
    "test_df_enriched = test_df_enriched.merge(avg_time_diff, on='ID', how='left')\n",
    "\n",
    "# Fill missing values for IDs that did not have event data\n",
    "test_df_enriched.fillna({'event_count': 0, 'unique_sources': 0, 'avg_time_diff':0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ID  LABEL\n",
      "0   17547      0\n",
      "1  140449      1\n",
      "2  182658      1\n",
      "3  149652      1\n",
      "4  106304      1\n"
     ]
    }
   ],
   "source": [
    "# Assuming `test_df_enriched` is your test dataset and `clf` is your trained LightGBM model\n",
    "\n",
    "# Convert the 'ID' column from float to int\n",
    "test_df_enriched['ID'] = test_df_enriched['ID'].astype(int)\n",
    "\n",
    "# Separate out the 'ID' column and features from the 'test' dataset\n",
    "#test_features = test_df_enriched.drop('ID', axis=1)\n",
    "test_features = test_df_enriched.drop('ID', axis=1)\n",
    "\n",
    "# Impute the 'test' dataset using the fitted SimpleImputer\n",
    "# Note: We use .transform() here, NOT .fit_transform()\n",
    "test_imputed = imputer.transform(test_features)\n",
    "\n",
    "# Predict the labels using the trained LightGBM model\n",
    "predicted_labels = clf.predict(test_imputed)\n",
    "\n",
    "# For binary classification, LightGBM might output probabilities. If so, convert these to 0 or 1\n",
    "# Uncomment the following line if your model outputs probabilities\n",
    "predicted_labels = np.round(predicted_labels).astype(int)\n",
    "\n",
    "# Create a DataFrame with 'ID' and the predicted 'label'\n",
    "results_df = pd.DataFrame({\n",
    "    'ID': test_df_enriched['ID'],\n",
    "    'LABEL': predicted_labels\n",
    "})\n",
    "\n",
    "# Optionally, convert the 'LABEL' column to the desired format, e.g., int\n",
    "# results_df['LABEL'] = results_df['LABEL'].astype(int)\n",
    "\n",
    "# Output the DataFrame with 'ID' and 'label'\n",
    "# Save to a CSV file\n",
    "results_df.to_csv('/Users/mohan/NEU/Projects_and_Hackathon/stay-or-stray/Submissions/my_submission_39_lGBM_ADAS_mdn_0.01LR_bgdt_wo_fold_2200BR_300NL.csv', index=False)\n",
    "\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(results_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
